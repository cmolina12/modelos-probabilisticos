{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b42647c",
   "metadata": {},
   "source": [
    "## Problema 1: Héroes y villanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ddf8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importaciones necesarias\n",
    "from jmarkov.mdp.dtmdp import dtmdp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d51e5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir estados\n",
    "estados = np.array([\"Ataque Centro\", \"Ataque Suburbio\"])\n",
    "\n",
    "# definir acciones\n",
    "acciones = np.array([\"Ir a Centro\", \"Ir a Suburbio\"])\n",
    "\n",
    "\n",
    "# definir diccionario de matrices (cada matriz asociada a su acción correspondiente)\n",
    "matriz_ir_centro = np.array([[0.3, 0.7], \n",
    "                            [0.8, 0.2]])\n",
    "matriz_ir_sub = np.array([[0.5, 0.5], \n",
    "                          [0.6, 0.4]])\n",
    "\n",
    "matrices = {}\n",
    "\n",
    "matrices[\"Ir a Centro\"] = matriz_ir_centro\n",
    "matrices[\"Ir a Suburbio\"] = matriz_ir_sub\n",
    "\n",
    "# derinir retornos: filas -> estados y columnas -> acciones\n",
    "retornos = np.array([[0.3, 0.5],\n",
    "                     [0.8, 0.4]])\n",
    "\n",
    "# definir un factor de descuento\n",
    "beta = 0.9\n",
    "\n",
    "# Creamos el objeto mdp con la librería jmarkov\n",
    "mdp_heroes = dtmdp(estados, acciones, matrices, retornos, beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dedf69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolver el mdp con el método .solve(): esto nos devuelve el valor esperado al seguir la política óptima en cada estado, y la política óptima para cada estado\n",
    "value_functions, optimal_policy = mdp_heroes.solve(0, minimize=False, method=\"policy_iteration\") # \"value_iteration\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad49b52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.06299207, 6.29921255])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc3bef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.str_('Ataque Centro'): np.str_('Ir a Suburbio'),\n",
       " np.str_('Ataque Suburbio'): np.str_('Ir a Centro')}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51e44f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.153846100216111)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcular el valor esperado de la política óptima (en total)\n",
    "mdp_heroes.expected_policy_value(value_functions, optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536c917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.8, 0.2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener la matriz de transición de la política óptima\n",
    "mdp_heroes.policy_transition_matrix(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd9cf3",
   "metadata": {},
   "source": [
    "## Problema 2: Los estudiantes de Los Alpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d90d9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_1 = np.array([[1,0,0,0],\n",
    "                [0.05,0.7,0.15,0.1],\n",
    "                [0,0.2,0.5,0.3],\n",
    "                [0,0,0.1,0.9]])\n",
    "\n",
    "P_2 = np.array([[0.6,0.4,0,0],\n",
    "                [0.25,0.6,0.1,0.05],\n",
    "                [0.1,0.3,0.5,0.1],\n",
    "                [0,0.05,0.25,0.7]])\n",
    "\n",
    "P_3 = np.array([[0.9,0.1,0,0],\n",
    "                [0.5,0.5,0,0],\n",
    "                [0.2,0.3,0.5,0],\n",
    "                [0,0,1,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd43dd",
   "metadata": {},
   "source": [
    "Completar todos los pasos, como el problema 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb974bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[1.  , 0.  , 0.  , 0.  ],\n",
       "        [0.05, 0.7 , 0.15, 0.1 ],\n",
       "        [0.  , 0.2 , 0.5 , 0.3 ],\n",
       "        [0.  , 0.  , 0.1 , 0.9 ]]),\n",
       " 2: array([[0.6 , 0.4 , 0.  , 0.  ],\n",
       "        [0.25, 0.6 , 0.1 , 0.05],\n",
       "        [0.1 , 0.3 , 0.5 , 0.1 ],\n",
       "        [0.  , 0.05, 0.25, 0.7 ]]),\n",
       " 3: array([[0.9, 0.1, 0. , 0. ],\n",
       "        [0.5, 0.5, 0. , 0. ],\n",
       "        [0.2, 0.3, 0.5, 0. ],\n",
       "        [0. , 0. , 1. , 0. ]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 0.9\n",
    "\n",
    "estados = np.array(['E', 'B', 'P', 'R'])\n",
    "\n",
    "acciones = np.array([i for i in range(1,4)])\n",
    "\n",
    "retornos = np.array([[-1000, 3, 0],\n",
    "                    [4, 2, -10],\n",
    "                    [3, -5, -10],\n",
    "                    [-3, -15, -1000]])\n",
    "matrices = {}\n",
    "matrices[1] = P_1\n",
    "matrices[2] = P_2\n",
    "matrices[3] = P_3\n",
    "matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ad3d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_estudiantes = dtmdp(estados, acciones, matrices, retornos, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ad9c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolver el mdp con el método .solve(): esto nos devuelve el valor esperado al seguir la política óptima en cada estado, y la política óptima para cada estado\n",
    "value_functions, optimal_policy = mdp_estudiantes.solve(0, minimize=False, method=\"policy_iteration\") # \"value_iteration\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31131ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.54413104,  10.25083408,   1.37878465, -15.13636513])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "693e7ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.str_('E'): np.int64(2),\n",
       " np.str_('B'): np.int64(2),\n",
       " np.str_('P'): np.int64(1),\n",
       " np.str_('R'): np.int64(1)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30fc478c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-4.146341393645345)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_estudiantes.expected_policy_value(value_functions, optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50b7be59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6 , 0.4 , 0.  , 0.  ],\n",
       "       [0.25, 0.6 , 0.1 , 0.05],\n",
       "       [0.  , 0.2 , 0.5 , 0.3 ],\n",
       "       [0.  , 0.  , 0.1 , 0.9 ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_estudiantes.policy_transition_matrix(optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
